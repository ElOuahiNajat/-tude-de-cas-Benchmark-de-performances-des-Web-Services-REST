# Benchmark â€” Web Services REST (Jersey / Spring MVC / Spring Data REST)

**RÃ©sumÃ© :**  
Ce projet compare les performances de trois variantes REST Java :
- **A - Jersey** : implÃ©mentation JAX-RS (Grizzly).
- **C - Spring MVC** : Spring Boot avec contrÃ´leurs MVC classiques.
- **D - Spring Data REST** : exposition automatique des entitÃ©s via Spring Data REST.

Les tests de charge sont rÃ©alisÃ©s avec **Apache JMeter**. Les mÃ©triques applicatives (Micrometer) sont exportÃ©es vers **Prometheus** et affichÃ©es dans **Grafana**. Des collectors supplÃ©mentaires peuvent exporter vers **InfluxDB** si nÃ©cessaire. Le dÃ©ploiement et l'orchestration se font via **Docker / docker-compose**.

---

## FonctionnalitÃ©s principales
- 3 variantes REST implÃ©mentÃ©es et packagÃ©es.
- ScÃ©narios JMeter fournis (scripts `.jmx`) pour charges lÃ©gÃ¨res, mixtes et lourdes.
- Collecte de mÃ©triques via Micrometer â†’ Prometheus.
- Dashboards Grafana prÃ©configurÃ©s (CPU, mÃ©moire, latence, GC, threads, RPS).
- Docker Compose pour lancer rapidement lâ€™environnement complet.

---

## PrÃ©-requis
- Git
- Docker & Docker Compose (version rÃ©cente)
- Java 17+ (pour compiler les variantes si vous ne voulez pas utiliser les images fournies)
- Maven (si compilation locale)
- Apache JMeter (pour exÃ©cuter les tests localement)

---

## Installation & lancement (rapide)

```bash
# cloner le dÃ©pÃ´t
git clone https://github.com/ElOuahiNajat/-tude-de-cas-Benchmark-de-performances-des-Web-Services-REST.git
cd -tude-de-cas-Benchmark-de-performances-des-Web-Services-REST

# lancer l'environnement avec docker-compose (ex : compose dÃ©jÃ  fourni)
docker compose up --build

```
<img width="1033" height="565" alt="image" src="https://github.com/user-attachments/assets/d9784e36-ea4f-496d-8a31-2f9f26b22d45" />
## ğŸ§© 3. Structure du projet

Lâ€™architecture du projet est conÃ§ue de maniÃ¨re modulaire afin dâ€™isoler chaque implÃ©mentation REST tout en mutualisant les Ã©lÃ©ments communs (scripts, configuration et monitoring).  
Chaque module peut Ãªtre exÃ©cutÃ© et analysÃ© indÃ©pendamment pour une comparaison prÃ©cise des performances.
projet/
â”œâ”€â”€ variant-a-jersey/           # ImplÃ©mentation JAX-RS (Jersey + Grizzly) - lÃ©gÃ¨re et modulaire
â”œâ”€â”€ variant-c-spring-mvc/       # ImplÃ©mentation REST via Spring MVC (@RestController) - Ã©quilibrÃ©e
â”œâ”€â”€ variant-d-spring-data-rest/ # ImplÃ©mentation Spring Data REST - rapide Ã  dÃ©velopper
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/java/com/example/  # Code source principal commun
â”‚   â””â”€â”€ test/java/com/example/  # Tests unitaires et dâ€™intÃ©gration
â”‚
â”œâ”€â”€ idea/                       # Configuration spÃ©cifique Ã  IntelliJ IDEA
â”œâ”€â”€ demo/                       # Fichiers et exemples de dÃ©monstration
â”œâ”€â”€ jmeter/                     # ScÃ©narios de tests de charge Apache JMeter (.jmx)
â”‚
â”œâ”€â”€ pom.xml                     # Fichier Maven principal pour la compilation et la gestion des dÃ©pendances
â”œâ”€â”€ docker-compose.yml          # DÃ©ploiement des containers (Base de donnÃ©es, Prometheus, Grafana, API)
â”œâ”€â”€ prometheus.yml              # Configuration de Prometheus pour lâ€™export des mÃ©triques
â”‚
â”œâ”€â”€ .gitignore                  # Liste des fichiers ignorÃ©s par Git
â”œâ”€â”€ README.md                   # Documentation principale du projet
â”œâ”€â”€ Compte Rendu Benchmark.pdf  # Rapport dÃ©taillÃ© dâ€™analyse comparative des performances
â”œâ”€â”€ category_ids.csv            # DonnÃ©es de test - identifiants des catÃ©gories
â”œâ”€â”€ item_ids.csv                # DonnÃ©es de test - identifiants des produits
â””â”€â”€ [autres fichiers]           # Scripts ou configurations supplÃ©mentaires

<img width="728" height="790" alt="image" src="https://github.com/user-attachments/assets/2e7b7200-0c95-409d-8dc0-59053460e063" />

## ğŸ³ 4. Conteneurs Docker

Lâ€™environnement de benchmark est entiÃ¨rement orchestrÃ© Ã  lâ€™aide de **Docker Compose**, garantissant une exÃ©cution reproductible et isolÃ©e des diffÃ©rents services.  
Chaque conteneur joue un rÃ´le spÃ©cifique dans la collecte, le stockage et la visualisation des performances.

ğŸ“ˆ Prometheus  â†’  Collecte les mÃ©triques exposÃ©es par les applications via Micrometer
ğŸ“Š Grafana     â†’  Visualise en temps rÃ©el les donnÃ©es issues de Prometheus et InfluxDB
ğŸ§® InfluxDB    â†’  Stocke les rÃ©sultats bruts des tests de charge JMeter
ğŸ§° API REST    â†’  Lâ€™une des trois variantes (Jersey, Spring MVC, Spring Data REST) testÃ©e
<img width="945" height="524" alt="image" src="https://github.com/user-attachments/assets/65589de0-b851-4f64-b20c-30529568dbc2" />
<img width="945" height="567" alt="image" src="https://github.com/user-attachments/assets/7f229f47-cbf7-45da-beaf-7d311393ba45" />
<img width="945" height="556" alt="image" src="https://github.com/user-attachments/assets/b3213867-5432-4504-8e87-9e52a6deddc0" />

## ğŸ“‚ 5. Jeu de donnÃ©es initial â€” DataSeeder

Le benchmark sâ€™appuie sur un **jeu de donnÃ©es rÃ©aliste** gÃ©nÃ©rÃ© automatiquement par la classe `DataSeeder`.  
Lâ€™objectif est de simuler un environnement applicatif proche dâ€™une vraie application e-commerce avec un volume important dâ€™enregistrements et des relations complexes.

---

### âš™ï¸ GÃ©nÃ©ration du dataset

Le script `DataSeeder.java` insÃ¨re un **volume consÃ©quent de donnÃ©es** dans PostgreSQL pour reproduire un contexte rÃ©el :

| Ã‰lÃ©ment                     | DÃ©tail                                   |
|------------------------------|-----------------------------------------|
| Nombre de catÃ©gories         | 2 000 (Category)                        |
| Nombre dâ€™items par catÃ©gorie | 50 (Item)                                |
| Total dâ€™items gÃ©nÃ©rÃ©s        | 100 000                                  |
| Taille moyenne des descriptions | 5 120 caractÃ¨res (~5 Ko par item)    |
| Flush batch                  | 5 000 entitÃ©s (optimisation JPA / mÃ©moire) |

---

### ğŸ“œ Description du fonctionnement

Le seeder utilise **JPA (Jakarta Persistence)** via un `EntityManager` configurÃ© avec `FlushModeType.COMMIT` afin de trouver un Ã©quilibre entre **performance et cohÃ©rence**.

#### 1ï¸âƒ£ CrÃ©ation des catÃ©gories
- Boucle dâ€™insertion de **2 000 entitÃ©s `Category`**.
- Nettoyage rÃ©gulier du contexte de persistance avec `em.flush()` et `em.clear()` tous les **500 enregistrements** pour limiter la consommation mÃ©moire.

#### 2ï¸âƒ£ CrÃ©ation des items
- Boucle imbriquÃ©e gÃ©nÃ©rant **50 `Item` par catÃ©gorie**.
- RÃ©fÃ©rence directe de la catÃ©gorie via `em.getReference(Category.class, cid)` pour Ã©viter les rechargements inutiles.
- Flush automatique tous les **5 000 items** pour rÃ©duire la consommation mÃ©moire.

#### 3ï¸âƒ£ Attributs simulÃ©s
- Champs : `sku`, `name`, `price`, `stock`, `description`, `category`.
- Les descriptions sont gÃ©nÃ©rÃ©es via `generateLorem(5120)` pour simuler un **corps JSON dâ€™environ 5 Ko**, utile dans les scÃ©narios POST/PUT lourds (`HeavyBody`).

---

### ğŸ“Š Objectifs du dataset
- Reproduire des **volumes comparables Ã  un environnement e-commerce rÃ©el**.
- Ã‰valuer les performances sur :
  - Relations **N:1 / 1:N** (Category â†’ Item)
  - RequÃªtes **JOIN** et filtrÃ©es
  - Gestion des **corps JSON volumineux** dans les opÃ©rations dâ€™Ã©criture
  - Comportement de lâ€™application sous **charge intensive** (scÃ©narios JMeter)

---

ğŸ’¡ **Astuce :**  
Ce dataset permet de tester efficacement :
- La scalabilitÃ© des diffÃ©rentes implÃ©mentations REST
- Lâ€™impact des flush batch sur la mÃ©moire et le temps de traitement
- Les performances des endpoints CRUD avec de gros volumes de donnÃ©es
  
<img width="745" height="211" alt="image" src="https://github.com/user-attachments/assets/dc091eb8-ccba-4dd3-9dba-6e1003a7eef9" />
<img width="550" height="244" alt="image" src="https://github.com/user-attachments/assets/d45d5f54-8eb8-497b-8814-6ff5e531995d" />


